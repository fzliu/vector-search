{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Search Using Milvus and SentenceTransformers\n",
    "In this example we are going to be going over a Wikipedia article search using Milvus and and the SentenceTransformers library. The dataset we are searching through is the Wikipedia-Movie-Plots Dataset found on [Kaggle](https://www.kaggle.com/datasets/jrobischon/wikipedia-movie-plots). For this example we have rehosted the data in a public google drive.\n",
    "\n",
    "Lets get started.\n",
    "\n",
    "## Installing Requirements\n",
    "For this example we are going to be using `pymilvus` to connect to use Milvus, `sentence-transformers` to connect to embed the movie plots, and `gdown` to download the example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymilvus in /Users/fzliu/.pyenv/lib/python3.8/site-packages (2.2.1)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 6.0 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: gdown in /Users/fzliu/.pyenv/lib/python3.8/site-packages (4.3.1)\n",
      "Requirement already satisfied: mmh3<=3.0.0,>=2.0 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from pymilvus) (3.0.0)\n",
      "Requirement already satisfied: grpcio<=1.48.0,>=1.47.0 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from pymilvus) (1.47.2)\n",
      "Requirement already satisfied: grpcio-tools<=1.48.0,>=1.47.0 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from pymilvus) (1.47.2)\n",
      "Requirement already satisfied: ujson<=5.4.0,>=2.0.0 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from pymilvus) (5.1.0)\n",
      "Requirement already satisfied: pandas>=1.2.4 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from pymilvus) (1.4.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from sentence-transformers) (4.25.1)\n",
      "Requirement already satisfied: tqdm in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from sentence-transformers) (4.62.3)\n",
      "Requirement already satisfied: torch>=1.6.0 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from sentence-transformers) (1.13.1)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.14.1-cp38-cp38-macosx_10_9_x86_64.whl (1.4 MB)\n",
      "Requirement already satisfied: numpy in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from sentence-transformers) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from sentence-transformers) (1.0.2)\n",
      "Requirement already satisfied: scipy in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from sentence-transformers) (1.9.3)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 19.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from sentence-transformers) (0.1.96)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from sentence-transformers) (0.11.1)\n",
      "Requirement already satisfied: six in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from gdown) (1.16.0)\n",
      "Requirement already satisfied: requests[socks] in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from gdown) (2.26.0)\n",
      "Requirement already satisfied: filelock in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from gdown) (3.3.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from gdown) (4.10.0)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from grpcio-tools<=1.48.0,>=1.47.0->pymilvus) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from grpcio-tools<=1.48.0,>=1.47.0->pymilvus) (49.2.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from pandas>=1.2.4->pymilvus) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from pandas>=1.2.4->pymilvus) (2.8.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from beautifulsoup4->gdown) (2.3.1)\n",
      "Requirement already satisfied: joblib in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from nltk->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from nltk->sentence-transformers) (8.0.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from requests[socks]->gdown) (2.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from requests[socks]->gdown) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from requests[socks]->gdown) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from requests[socks]->gdown) (3.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (3.0.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/fzliu/.pyenv/lib/python3.8/site-packages (from torchvision->sentence-transformers) (9.1.1)\n",
      "Building wheels for collected packages: sentence-transformers\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125919 sha256=4a16738e15b53a4f00b540de1ef0b22c31d87a23ebe3c22714e92cfa653494d8\n",
      "  Stored in directory: /Users/fzliu/Library/Caches/pip/wheels/5e/6f/8c/d88aec621f3f542d26fac0342bef5e693335d125f4e54aeffe\n",
      "Successfully built sentence-transformers\n",
      "Installing collected packages: torchvision, nltk, sentence-transformers\n",
      "Successfully installed nltk-3.8.1 sentence-transformers-2.2.2 torchvision-0.14.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/Users/fzliu/.pyenv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install pymilvus sentence-transformers gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabbing the Data\n",
    "We are going to use `gdown` to grab the zip from Google Drive and then decompress it with the built in `zipfile` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access denied with the following error:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " \tCannot retrieve the public link of the file. You may need to change\n",
      "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
      "\n",
      "You may still be able to access the file from the browser:\n",
      "\n",
      "\t https://drive.google.com/uc?id=11ISS45aO2ubNCGaC3Lvd3D7NT8Y7MeO8 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "url = 'https://drive.google.com/uc?id=11ISS45aO2ubNCGaC3Lvd3D7NT8Y7MeO8'\n",
    "output = './movies.zip'\n",
    "gdown.download(url, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"./movies.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Arguments\n",
    "Here we can find the main arguments that need to be modified for running with your own accounts. Beside each is a description of what it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Milvus Setup Arguments\n",
    "import os\n",
    "COLLECTION_NAME = 'movies_db'  # Collection name\n",
    "DIMENSION = 384  # Embeddings size\n",
    "URI=os.getenv('VECTOR_DB_URL')  # Endpoint URI obtained from Zilliz Cloud\n",
    "USER='db_admin'  # Username specified when you created this database\n",
    "PASSWORD=os.getenv('VECTOR_DB_PASS')  # Password set for that account\n",
    "\n",
    "# Inference Arguments\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Search Arguments\n",
    "TOP_K = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Milvus\n",
    "At this point we are going to begin setting up Milvus. The steps are as follows:\n",
    "\n",
    "1. Connect to the Milvus instance using the provided URI.\n",
    "2. If the collection already exists, drop it.\n",
    "3. Create the collection that holds the id, title of the movie, and the plot embedding.\n",
    "4. Create an index on the newly created collection and load it into memory.\n",
    "\n",
    "Once these steps are done the collection is ready to be inserted into and searched. Any data added will be indexed automatically and be available to search immidiately. If the data is very fresh, the search might be slower as brute force searching will be used on data that is still in process of getting indexed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections\n",
    "\n",
    "# Connect to Milvus Database\n",
    "connections.connect(uri=URI, user=USER, password=PASSWORD, secure=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import utility\n",
    "\n",
    "# Remove any previous collections with the same name\n",
    "if utility.has_collection(COLLECTION_NAME):\n",
    "    utility.drop_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import FieldSchema, CollectionSchema, DataType, Collection\n",
    "\n",
    "\n",
    "# Create collection which includes the id, title, and embedding.\n",
    "fields = [\n",
    "    FieldSchema(name='id', dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name='title', dtype=DataType.VARCHAR, max_length=200),  # VARCHARS need a maximum length, so for this example they are set to 200 characters\n",
    "    FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, dim=DIMENSION)\n",
    "]\n",
    "schema = CollectionSchema(fields=fields)\n",
    "collection = Collection(name=COLLECTION_NAME, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an IVF_FLAT index for collection.\n",
    "index_params = {\n",
    "    'metric_type':'L2',\n",
    "    'index_type':\"AUTOINDEX\",\n",
    "    'params':{}\n",
    "}\n",
    "collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "collection.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting the Data\n",
    "In these next few steps we will be: \n",
    "1. Loading the data.\n",
    "2. Embedding the plot text data using SentenceTransformers.\n",
    "3. Inserting the data into Milvus.\n",
    "\n",
    "For this example we are going using SentenceTransformers miniLM model to create embeddings of the plot text. This model returns 384-dim embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 22:14:24.344794: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b50c848941f4414adab966fd59299c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0ae3f2576d4aec91802fb655f12449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807a18e05dee497a94a9900c2da6ec5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffedaf15af246a79bf3cb42e7b20183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f714013ca3b4a48af6a99366381ac85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611602581df14d8ea6aaf1a33fe9f27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5112c98eff54c0e87f1f1c2dd959745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5fb049ebe874582bae1e9c5b43ab5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a823a2a47354a3a84819accbb6fa32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4506219c5ec4eee9e175bffc91e39f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb1ea8f8ab74d2c9e6e7e922c688a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d9317fd6444d80a3fc5b2b17385d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba10310bd6a549f999df8f6f52dab638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa25b761121d4e15b2122e6e518fb031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "transformer = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Extract the movie titles\n",
    "def csv_load(file):\n",
    "    with open(file, newline='') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            if '' in (row[1], row[7]):\n",
    "                continue\n",
    "            yield (row[1], row[7])\n",
    "\n",
    "\n",
    "# Extract embeding from text using SentenceEmbeddings\n",
    "def embed_insert(data):\n",
    "    embeds = transformer.encode(data[1]) \n",
    "    ins = [\n",
    "            data[0],\n",
    "            [x for x in embeds]\n",
    "    ]\n",
    "    collection.insert(ins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "data_batch = [[],[]]\n",
    "\n",
    "for title, plot in csv_load('./movies/plots.csv'):\n",
    "    data_batch[0].append(title)\n",
    "    data_batch[1].append(plot)\n",
    "    if len(data_batch[0]) % BATCH_SIZE == 0:\n",
    "        embed_insert(data_batch)\n",
    "        data_batch = [[],[]]\n",
    "\n",
    "# Embed and insert the remainder\n",
    "if len(data_batch[0]) != 0:\n",
    "    embed_insert(data_batch)\n",
    "\n",
    "# Call a flush to index any unsealed segments.\n",
    "collection.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing the Search\n",
    "With all the data inserted into Milvus we can start performing our searches. In this example we are going to search for movies based on the plot. Because we are doing a batch search, the search time is shared across the movie searches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for titles that closest match these phrases.\n",
    "search_terms = ['A movie about cars', 'A movie about monsters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: A movie about cars\n",
      "Search Time: 0.04272913932800293\n",
      "Results:\n",
      "Red Line 7000 ---- 0.9104408621788025\n",
      "The Mysterious Mr. Valentine ---- 0.9127437472343445\n",
      "Tomboy ---- 0.9254708290100098\n",
      "\n",
      "Title: A movie about monsters\n",
      "Search Time: 0.04272913932800293\n",
      "Results:\n",
      "Monster Hunt ---- 0.8105474710464478\n",
      "The Astro-Zombies ---- 0.8998500108718872\n",
      "Wild Country ---- 0.9238440990447998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search the database based on input text\n",
    "def embed_search(data):\n",
    "    embeds = transformer.encode(data) \n",
    "    return [x for x in embeds]\n",
    "\n",
    "search_data = embed_search(search_terms)\n",
    "\n",
    "start = time.time()\n",
    "res = collection.search(\n",
    "    data=search_data,  # Embeded search value\n",
    "    anns_field=\"embedding\",  # Search across embeddings\n",
    "    param={},\n",
    "    limit = TOP_K,  # Limit to top_k results per search\n",
    "    output_fields=['title']  # Include title field in result\n",
    ")\n",
    "end = time.time()\n",
    "\n",
    "for hits_i, hits in enumerate(res):\n",
    "    print('Title:', search_terms[hits_i])\n",
    "    print('Search Time:', end-start)\n",
    "    print('Results:')\n",
    "    for hit in hits:\n",
    "        print( hit.entity.get('title'), '----', hit.distance)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "0b4612e1e455c31d4e01543291f0d5bf5b305f05f1c8f517b55c0e2c9e4a2791"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
